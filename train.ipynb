{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/train'\n",
    "val_dir = 'data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n",
    "# emotion_model.load_weights('emotion_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.ocl.setUseOpenCL(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vignesh\\AppData\\Local\\Temp\\ipykernel_3828\\3870905249.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  emotion_model_info = emotion_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 478s 1s/step - loss: 1.8034 - accuracy: 0.2575 - val_loss: 1.7218 - val_accuracy: 0.3514\n"
     ]
    }
   ],
   "source": [
    "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "emotion_model_info = emotion_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=28709 // 64,\n",
    "        epochs=1,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=7178 // 64)\n",
    "emotion_model.save_weights('emotion_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Surprised\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Surprised\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vignesh\\Documents\\ML_jcomp\\emoji-creator-project-code\\train.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vignesh/Documents/ML_jcomp/emoji-creator-project-code/train.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m bounding_box \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mCascadeClassifier(\u001b[39m'\u001b[39m\u001b[39mopencv-master/data/haarcascades/haarcascade_frontalface_default.xml\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vignesh/Documents/ML_jcomp/emoji-creator-project-code/train.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m gray_frame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Vignesh/Documents/ML_jcomp/emoji-creator-project-code/train.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m num_faces \u001b[39m=\u001b[39m bounding_box\u001b[39m.\u001b[39;49mdetectMultiScale(frame,scaleFactor\u001b[39m=\u001b[39;49m\u001b[39m1.3\u001b[39;49m, minNeighbors\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vignesh/Documents/ML_jcomp/emoji-creator-project-code/train.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m (x, y, w, h) \u001b[39min\u001b[39;00m num_faces:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vignesh/Documents/ML_jcomp/emoji-creator-project-code/train.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     cv2\u001b[39m.\u001b[39mrectangle(frame, (x, y\u001b[39m-\u001b[39m\u001b[39m50\u001b[39m), (x\u001b[39m+\u001b[39mw, y\u001b[39m+\u001b[39mh\u001b[39m+\u001b[39m\u001b[39m10\u001b[39m), (\u001b[39m255\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m), \u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start the webcam feed\n",
    "cap = cv2.VideoCapture(1)\n",
    "while True:\n",
    "    # Find haar cascade to draw bounding box around face\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    bounding_box = cv2.CascadeClassifier('opencv-master/data/haarcascades/haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    num_faces = bounding_box.detectMultiScale(frame,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = frame[y:y + h, x:x + w]\n",
    "        temp1 = cv2.resize(roi_gray_frame, (48, 48))\n",
    "        temp = np.expand_dims(cv2.resize(temp1,(48,48)), -1)\n",
    "        temp.shape=(3,48,48,1)\n",
    "        emotion_prediction = emotion_model.predict(temp)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        print(emotion_dict[maxindex])\n",
    "        #cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Video', cv2.resize(frame,(1200,860),interpolation = cv2.INTER_CUBIC))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f7f5ee8dd051df70ed33d70ae4e9eb9a1d2c4e6eb38196cc61a2aa1a8f32ae9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
